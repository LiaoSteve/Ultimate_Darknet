 OpenCV version: 3.2.0
 0 : compute_capability = 750, cudnn_half = 1, GPU: GeForce RTX 2080 Ti 
net.optimized_memory = 0 
mini_batch = 1, batch = 16, time_steps = 1, train = 0 
   layer   filters  size/strd(dil)      input                output
   0 conv     32       3 x 3/ 2    864 x 864 x   3 ->  432 x 432 x  32 0.322 BF
   1 conv     32       1 x 1/ 1    432 x 432 x  32 ->  432 x 432 x  32 0.382 BF
   2 conv     32/  32  3 x 3/ 1    432 x 432 x  32 ->  432 x 432 x  32 0.107 BF
   3 avg                           432 x 432 x  32 ->     32
   4 conv      8       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x   8 0.000 BF
   5 conv     32       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x  32 0.000 BF
   6 scale Layer: 2
   7 conv     16       1 x 1/ 1    432 x 432 x  32 ->  432 x 432 x  16 0.191 BF
   8 conv     96       1 x 1/ 1    432 x 432 x  16 ->  432 x 432 x  96 0.573 BF
   9 conv     96/  96  3 x 3/ 2    432 x 432 x  96 ->  216 x 216 x  96 0.081 BF
  10 avg                           216 x 216 x  96 ->     96
  11 conv     16       1 x 1/ 1      1 x   1 x  96 ->    1 x   1 x  16 0.000 BF
  12 conv     96       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x  96 0.000 BF
  13 scale Layer: 9
  14 conv     24       1 x 1/ 1    216 x 216 x  96 ->  216 x 216 x  24 0.215 BF
  15 conv    144       1 x 1/ 1    216 x 216 x  24 ->  216 x 216 x 144 0.322 BF
  16 conv    144/ 144  3 x 3/ 1    216 x 216 x 144 ->  216 x 216 x 144 0.121 BF
  17 avg                           216 x 216 x 144 ->    144
  18 conv      8       1 x 1/ 1      1 x   1 x 144 ->    1 x   1 x   8 0.000 BF
  19 conv    144       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x 144 0.000 BF
  20 scale Layer: 16
  21 conv     24       1 x 1/ 1    216 x 216 x 144 ->  216 x 216 x  24 0.322 BF
  22 dropout    p = 0.000        1119744  ->   1119744
  23 Shortcut Layer: 14,  wt = 0, wn = 0, outputs: 216 x 216 x  24 0.001 BF
  24 conv    144       1 x 1/ 1    216 x 216 x  24 ->  216 x 216 x 144 0.322 BF
  25 conv    144/ 144  5 x 5/ 2    216 x 216 x 144 ->  108 x 108 x 144 0.084 BF
  26 avg                           108 x 108 x 144 ->    144
  27 conv      8       1 x 1/ 1      1 x   1 x 144 ->    1 x   1 x   8 0.000 BF
  28 conv    144       1 x 1/ 1      1 x   1 x   8 ->    1 x   1 x 144 0.000 BF
  29 scale Layer: 25
  30 conv     40       1 x 1/ 1    108 x 108 x 144 ->  108 x 108 x  40 0.134 BF
  31 conv    192       1 x 1/ 1    108 x 108 x  40 ->  108 x 108 x 192 0.179 BF
  32 conv    192/ 192  5 x 5/ 1    108 x 108 x 192 ->  108 x 108 x 192 0.112 BF
  33 avg                           108 x 108 x 192 ->    192
  34 conv     16       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  16 0.000 BF
  35 conv    192       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 192 0.000 BF
  36 scale Layer: 32
  37 conv     40       1 x 1/ 1    108 x 108 x 192 ->  108 x 108 x  40 0.179 BF
  38 dropout    p = 0.000        466560  ->   466560
  39 Shortcut Layer: 30,  wt = 0, wn = 0, outputs: 108 x 108 x  40 0.000 BF
  40 conv    192       1 x 1/ 1    108 x 108 x  40 ->  108 x 108 x 192 0.179 BF
  41 conv    192/ 192  3 x 3/ 1    108 x 108 x 192 ->  108 x 108 x 192 0.040 BF
  42 avg                           108 x 108 x 192 ->    192
  43 conv     16       1 x 1/ 1      1 x   1 x 192 ->    1 x   1 x  16 0.000 BF
  44 conv    192       1 x 1/ 1      1 x   1 x  16 ->    1 x   1 x 192 0.000 BF
  45 scale Layer: 41
  46 conv     80       1 x 1/ 1    108 x 108 x 192 ->  108 x 108 x  80 0.358 BF
  47 conv    384       1 x 1/ 1    108 x 108 x  80 ->  108 x 108 x 384 0.717 BF
  48 conv    384/ 384  3 x 3/ 1    108 x 108 x 384 ->  108 x 108 x 384 0.081 BF
  49 avg                           108 x 108 x 384 ->    384
  50 conv     24       1 x 1/ 1      1 x   1 x 384 ->    1 x   1 x  24 0.000 BF
  51 conv    384       1 x 1/ 1      1 x   1 x  24 ->    1 x   1 x 384 0.000 BF
  52 scale Layer: 48
  53 conv     80       1 x 1/ 1    108 x 108 x 384 ->  108 x 108 x  80 0.717 BF
  54 dropout    p = 0.000        933120  ->   933120
  55 Shortcut Layer: 46,  wt = 0, wn = 0, outputs: 108 x 108 x  80 0.001 BF
  56 conv    384       1 x 1/ 1    108 x 108 x  80 ->  108 x 108 x 384 0.717 BF
  57 conv    384/ 384  3 x 3/ 1    108 x 108 x 384 ->  108 x 108 x 384 0.081 BF
  58 avg                           108 x 108 x 384 ->    384
  59 conv     24       1 x 1/ 1      1 x   1 x 384 ->    1 x   1 x  24 0.000 BF
  60 conv    384       1 x 1/ 1      1 x   1 x  24 ->    1 x   1 x 384 0.000 BF
  61 scale Layer: 57
  62 conv     80       1 x 1/ 1    108 x 108 x 384 ->  108 x 108 x  80 0.717 BF
  63 dropout    p = 0.000        933120  ->   933120
  64 Shortcut Layer: 55,  wt = 0, wn = 0, outputs: 108 x 108 x  80 0.001 BF
  65 conv    384       1 x 1/ 1    108 x 108 x  80 ->  108 x 108 x 384 0.717 BF
  66 conv    384/ 384  5 x 5/ 2    108 x 108 x 384 ->   54 x  54 x 384 0.056 BF
  67 avg                            54 x  54 x 384 ->    384
  68 conv     24       1 x 1/ 1      1 x   1 x 384 ->    1 x   1 x  24 0.000 BF
  69 conv    384       1 x 1/ 1      1 x   1 x  24 ->    1 x   1 x 384 0.000 BF
  70 scale Layer: 66
  71 conv    112       1 x 1/ 1     54 x  54 x 384 ->   54 x  54 x 112 0.251 BF
  72 conv    576       1 x 1/ 1     54 x  54 x 112 ->   54 x  54 x 576 0.376 BF
  73 conv    576/ 576  5 x 5/ 1     54 x  54 x 576 ->   54 x  54 x 576 0.084 BF
  74 avg                            54 x  54 x 576 ->    576
  75 conv     32       1 x 1/ 1      1 x   1 x 576 ->    1 x   1 x  32 0.000 BF
  76 conv    576       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 576 0.000 BF
  77 scale Layer: 73
  78 conv    112       1 x 1/ 1     54 x  54 x 576 ->   54 x  54 x 112 0.376 BF
  79 dropout    p = 0.000        326592  ->   326592
  80 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  54 x  54 x 112 0.000 BF
  81 conv    576       1 x 1/ 1     54 x  54 x 112 ->   54 x  54 x 576 0.376 BF
  82 conv    576/ 576  5 x 5/ 1     54 x  54 x 576 ->   54 x  54 x 576 0.084 BF
  83 avg                            54 x  54 x 576 ->    576
  84 conv     32       1 x 1/ 1      1 x   1 x 576 ->    1 x   1 x  32 0.000 BF
  85 conv    576       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 576 0.000 BF
  86 scale Layer: 82
  87 conv    112       1 x 1/ 1     54 x  54 x 576 ->   54 x  54 x 112 0.376 BF
  88 dropout    p = 0.000        326592  ->   326592
  89 Shortcut Layer: 80,  wt = 0, wn = 0, outputs:  54 x  54 x 112 0.000 BF
  90 conv    576       1 x 1/ 1     54 x  54 x 112 ->   54 x  54 x 576 0.376 BF
  91 conv    576/ 576  5 x 5/ 2     54 x  54 x 576 ->   27 x  27 x 576 0.021 BF
  92 avg                            27 x  27 x 576 ->    576
  93 conv     32       1 x 1/ 1      1 x   1 x 576 ->    1 x   1 x  32 0.000 BF
  94 conv    576       1 x 1/ 1      1 x   1 x  32 ->    1 x   1 x 576 0.000 BF
  95 scale Layer: 91
  96 conv    192       1 x 1/ 1     27 x  27 x 576 ->   27 x  27 x 192 0.161 BF
  97 conv    960       1 x 1/ 1     27 x  27 x 192 ->   27 x  27 x 960 0.269 BF
  98 conv    960/ 960  5 x 5/ 1     27 x  27 x 960 ->   27 x  27 x 960 0.035 BF
  99 avg                            27 x  27 x 960 ->    960
 100 conv     64       1 x 1/ 1      1 x   1 x 960 ->    1 x   1 x  64 0.000 BF
 101 conv    960       1 x 1/ 1      1 x   1 x  64 ->    1 x   1 x 960 0.000 BF
 102 scale Layer: 98
 103 conv    192       1 x 1/ 1     27 x  27 x 960 ->   27 x  27 x 192 0.269 BF
 104 dropout    p = 0.000        139968  ->   139968
 105 Shortcut Layer: 96,  wt = 0, wn = 0, outputs:  27 x  27 x 192 0.000 BF
 106 conv    960       1 x 1/ 1     27 x  27 x 192 ->   27 x  27 x 960 0.269 BF
 107 conv    960/ 960  5 x 5/ 1     27 x  27 x 960 ->   27 x  27 x 960 0.035 BF
 108 avg                            27 x  27 x 960 ->    960
 109 conv     64       1 x 1/ 1      1 x   1 x 960 ->    1 x   1 x  64 0.000 BF
 110 conv    960       1 x 1/ 1      1 x   1 x  64 ->    1 x   1 x 960 0.000 BF
 111 scale Layer: 107
 112 conv    192       1 x 1/ 1     27 x  27 x 960 ->   27 x  27 x 192 0.269 BF
 113 dropout    p = 0.000        139968  ->   139968
 114 Shortcut Layer: 105,  wt = 0, wn = 0, outputs:  27 x  27 x 192 0.000 BF
 115 conv    960       1 x 1/ 1     27 x  27 x 192 ->   27 x  27 x 960 0.269 BF
 116 conv    960/ 960  5 x 5/ 1     27 x  27 x 960 ->   27 x  27 x 960 0.035 BF
 117 avg                            27 x  27 x 960 ->    960
 118 conv     64       1 x 1/ 1      1 x   1 x 960 ->    1 x   1 x  64 0.000 BF
 119 conv    960       1 x 1/ 1      1 x   1 x  64 ->    1 x   1 x 960 0.000 BF
 120 scale Layer: 116
 121 conv    192       1 x 1/ 1     27 x  27 x 960 ->   27 x  27 x 192 0.269 BF
 122 dropout    p = 0.000        139968  ->   139968
 123 Shortcut Layer: 114,  wt = 0, wn = 0, outputs:  27 x  27 x 192 0.000 BF
 124 conv    960       1 x 1/ 1     27 x  27 x 192 ->   27 x  27 x 960 0.269 BF
 125 conv    960/ 960  3 x 3/ 1     27 x  27 x 960 ->   27 x  27 x 960 0.013 BF
 126 avg                            27 x  27 x 960 ->    960
 127 conv     64       1 x 1/ 1      1 x   1 x 960 ->    1 x   1 x  64 0.000 BF
 128 conv    960       1 x 1/ 1      1 x   1 x  64 ->    1 x   1 x 960 0.000 BF
 129 scale Layer: 125
 130 conv    320       1 x 1/ 1     27 x  27 x 960 ->   27 x  27 x 320 0.448 BF
 131 conv   1280       1 x 1/ 1     27 x  27 x 320 ->   27 x  27 x1280 0.597 BF
 132 conv    256       1 x 1/ 1     27 x  27 x1280 ->   27 x  27 x 256 0.478 BF
 133 conv    256       3 x 3/ 1     27 x  27 x 256 ->   27 x  27 x 256 0.860 BF
 134 Shortcut Layer: 132,  wt = 0, wn = 0, outputs:  27 x  27 x 256 0.000 BF
 135 conv     21       1 x 1/ 1     27 x  27 x 256 ->   27 x  27 x  21 0.008 BF
 136 yolo
[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00
 137 route  133 		                           ->   27 x  27 x 256 
 138 conv    128       1 x 1/ 1     27 x  27 x 256 ->   27 x  27 x 128 0.048 BF
 139 upsample                 2x    27 x  27 x 128 ->   54 x  54 x 128
 140 Shortcut Layer: 90,  wt = 0, wn = 0, outputs:  54 x  54 x 128 0.000 BF
 (  54 x  54 x 128) + (  54 x  54 x 576) 
 141 conv    128       3 x 3/ 1     54 x  54 x 128 ->   54 x  54 x 128 0.860 BF
 142 Shortcut Layer: 139,  wt = 0, wn = 0, outputs:  54 x  54 x 128 0.000 BF
 143 Shortcut Layer: 90,  wt = 0, wn = 0, outputs:  54 x  54 x 128 0.000 BF
 (  54 x  54 x 128) + (  54 x  54 x 576) 
 144 conv     21       1 x 1/ 1     54 x  54 x 128 ->   54 x  54 x  21 0.016 BF
 145 yolo
[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00
Total BFLOPS 15.830 
avg_outputs = 1439228 
 Allocate additional workspace_size = 56.74 MB 
Loading weights from backup/enet-coco_best.weights...
 seen 64, trained: 224 K-images (3 Kilo-batches_64) 
Done! Loaded 146 layers from weights-file 

 calculation mAP (mean average precision)...
 Detection layer: 136 - type = 28 
 Detection layer: 145 - type = 28 

